{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display, Math, Latex\n",
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.8)\n",
    "s = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "1: implement a simple feef forward neural network to see how good it works on the dev set\n",
    "2: define the hyperparameters and try tuning them \n",
    "\n",
    "ideas:\n",
    "1: try modelling the shape using a rnn (maybe lstm) and then feed the results and alpha and Re to feed forward network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inputs\n",
    "#with open('polars.pickle', 'r') as fid:\n",
    "#    polars = pickle.load(fid)\n",
    "with open('af_points.pickle', 'rb') as fid:\n",
    "    af_data_dic = pickle.load(fid, encoding='latin1')\n",
    "with open('af_labels.pickle', 'rb') as fid:\n",
    "    af_label = pickle.load(fid, encoding='latin1')\n",
    "with open('label_afs.pickle', 'rb') as fid:\n",
    "    label_af = pickle.load(fid, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lamda = 0.05\n",
    "learning_rate = 0.0005\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "layers = [80,25,25,4]\n",
    "s_train, s_dev, s_tes = 0.94, 0.03, 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input\n",
    "data = pd.read_csv('raw_af_data.txt')\n",
    "index = data.index\n",
    "h = data.copy()\n",
    "# normalize re and alpha\n",
    "# ====================================\n",
    "mu_a = h['a'].mean()\n",
    "sigma_a = h['a'].std()\n",
    "re_max = h['re'].max()\n",
    "# ====================\n",
    "h['a'] = (h['a'] - mu_a) / sigma_a\n",
    "h['re'] = h['re'] / re_max\n",
    "# ====================================\n",
    "# shuffle the data three times\n",
    "h = h.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "h = h.reindex(np.random.permutation(h.index)).reset_index(drop=True)\n",
    "# third shuffle\n",
    "inputs_train = h.sample(frac=s_train)\n",
    "remaining = h.drop(inputs_train.index)\n",
    "inputs_train = inputs_train.reset_index(drop=True)\n",
    "inputs_dev = remaining.sample(frac=(s_dev/(1-s_train)))\n",
    "inputs_test = remaining.drop(inputs_dev.index).reset_index(drop=True)\n",
    "inputs_dev = inputs_dev.reset_index(drop=True)\n",
    "\n",
    "\n",
    "x_train = inputs_train[['af', 're', 'a']].values.transpose()\n",
    "y_train = inputs_train[['cl', 'cd', 'cdp', 'cm']].values.transpose()\n",
    "m_train = x_train.shape[1]\n",
    "\n",
    "x_dev = inputs_dev[['af', 're', 'a']].values.transpose()\n",
    "y_dev = inputs_dev[['cl', 'cd', 'cdp', 'cm']].values.transpose()\n",
    "m_dev = x_dev.shape[1]\n",
    "\n",
    "x_test = inputs_test[['af', 're', 'a']].values.transpose()\n",
    "y_test = inputs_test[['cl', 'cd', 'cdp', 'cm']].values.transpose()\n",
    "m_test = x_test.shape[1]\n",
    "\n",
    "# m_train + m_dev + m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = af_data_dic[label_af[0]]['input'].shape[1] + 2\n",
    "x = tf.placeholder('float64', shape=(n, None))\n",
    "y = tf.placeholder('float64', shape=(4, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 13:04:25.641305 140464238892864 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "W1 = tf.get_variable(\"W1\", (layers[0], n), \n",
    "                     initializer=tf.contrib.layers.xavier_initializer(seed=0), \n",
    "                     dtype=tf.float64)\n",
    "\n",
    "b1 = tf.get_variable(\"b1\", [layers[0], 1], initializer=tf.zeros_initializer(), dtype=tf.float64)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", [layers[1], layers[0]], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer(seed=0), \n",
    "                     dtype=tf.float64)\n",
    "\n",
    "b2 = tf.get_variable(\"b2\", [layers[1], 1], initializer=tf.zeros_initializer(), dtype=tf.float64)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", [layers[2], layers[1]], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer(seed=0), \n",
    "                     dtype=tf.float64)\n",
    "\n",
    "b3 = tf.get_variable(\"b3\", [layers[2], 1], initializer=tf.zeros_initializer(), dtype=tf.float64)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", [layers[3], layers[2]], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer(seed=0), \n",
    "                     dtype=tf.float64)\n",
    "\n",
    "b4 = tf.get_variable(\"b4\", [layers[3], 1], initializer=tf.zeros_initializer(), dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Prop\n",
    "Z1 = tf.add(tf.matmul(W1, x), b1)\n",
    "A1 = tf.nn.tanh(Z1)\n",
    "Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
    "A2 = tf.nn.tanh(Z2)\n",
    "Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
    "A3 = tf.nn.tanh(Z3) \n",
    "Z4 = tf.add(tf.matmul(W4, A3), b4)\n",
    "A4 = Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cost\n",
    "cost = tf.reduce_mean((y-A4)**2.)\n",
    "\n",
    "# frobenious regularization\n",
    "reg = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4) \n",
    "\n",
    "# compute reqularized loss\n",
    "cost = tf.reduce_mean(cost + lamda * reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0721 13:04:32.649280 140464238892864 deprecation.py:323] From /home/ariya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.182370\n",
      "Cost after epoch 1: 0.107940\n",
      "Cost after epoch 2: 0.107919\n",
      "Cost after epoch 3: 0.107907\n",
      "Cost after epoch 4: 0.107915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6ed7c4c1a8df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# convert the x_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mx_af\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mx_af\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maf_data_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_af\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_af\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mx_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mminibatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_af\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training \n",
    "seed = 10\n",
    "costs = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0.\n",
    "        n_batch = int(m_train/batch_size)\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(x_train, y_train, batch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_x, minibatch_y) = minibatch\n",
    "            # convert the x_s\n",
    "            x_af = minibatch_x[0,:].astype(int)\n",
    "            x_af = np.array([af_data_dic[label_af[i]]['input'].flatten() for i in x_af]).transpose()\n",
    "            x_temp = minibatch_x[1:3,:]\n",
    "            minibatch_x = np.concatenate((x_af, x_temp), axis=0)\n",
    "            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "            epoch_cost += minibatch_cost / n_batch\n",
    "\n",
    "        # Print the cost every epoch\n",
    "        if epoch % 1 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if epoch % 1 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.67000000e+02,  9.40000000e+01,  1.35000000e+02, ...,\n",
       "         3.67000000e+02,  9.60000000e+01,  1.34000000e+02],\n",
       "       [ 7.12328767e-01,  7.80821918e-01,  3.01369863e-01, ...,\n",
       "         4.10958904e-02,  6.98630137e-01,  1.36986301e-01],\n",
       "       [ 8.69089580e-01, -2.38664736e-01,  1.55078454e+00, ...,\n",
       "         6.98665839e-01,  1.69708753e-02, -1.60205466e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_af = x_dev[0,:].astype(int)\n",
    "x_af = np.array([af_data_dic[label_af[i]]['input'].flatten() for i in x_af]).transpose()\n",
    "x_temp = x_dev[1:3,:]\n",
    "x_dex_n = np.concatenate((x_af, x_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
