{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "class AirfoilPredictor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AirfoilPredictor, self).__init__()\n",
    "        self.net = {}\n",
    "        self.d = 326\n",
    "        self.lamda = 0.00001\n",
    "        self.eta = 0.001\n",
    "        self.beta_1 = 0.95\n",
    "        self.beta_2 = 0.999\n",
    "\n",
    "        self.l0  = torch.nn.Linear(800, 500, bias=True)\n",
    "        self.l01 = torch.nn.Linear(500, 200, bias=True)\n",
    "        self.l02 = torch.nn.Linear(200, 1, bias=True)\n",
    "\n",
    "        self.l1  = torch.nn.Linear(self.d, 800, bias=True)\n",
    "        self.l2  = torch.nn.Linear(800, 800, bias=True)\n",
    "        self.l3  = torch.nn.Linear(800, 800, bias=True)\n",
    "        self.l4  = torch.nn.Linear(800, 800, bias=True)\n",
    "        self.l5  = torch.nn.Linear(800, 800, bias=True)\n",
    "        self.l6  = torch.nn.Linear(800, 780, bias=True)\n",
    "        self.l7  = torch.nn.Linear(780, 721, bias=True)\n",
    "        self.l8  = torch.nn.Linear(721, 675, bias=True)\n",
    "        self.l9  = torch.nn.Linear(675, 625, bias=True)\n",
    "        self.l10 = torch.nn.Linear(625, 580, bias=True)\n",
    "        self.l11 = torch.nn.Linear(580, 540, bias=True)\n",
    "        self.l12 = torch.nn.Linear(540, 500, bias=True)\n",
    "        self.l13 = torch.nn.Linear(500, 440, bias=True)\n",
    "        self.l14 = torch.nn.Linear(440, 380, bias=True)\n",
    "        self.l15 = torch.nn.Linear(380, 350, bias=True)\n",
    "        self.l16 = torch.nn.Linear(350, 300, bias=True)\n",
    "        self.l17 = torch.nn.Linear(300, 280, bias=True)\n",
    "        self.l18 = torch.nn.Linear(280, 250, bias=True)\n",
    "        self.l19 = torch.nn.Linear(250, 200, bias=True)\n",
    "        self.l20 = torch.nn.Linear(200, 140, bias=True)\n",
    "        self.l21 = torch.nn.Linear(140, 120, bias=True)\n",
    "        self.l22 = torch.nn.Linear(120, 100, bias=True)\n",
    "        self.l23 = torch.nn.Linear(100, 1, bias=True)\n",
    "\n",
    "\n",
    "        # self.r5  = torch.nn.Linear(350, 300, bias=True)\n",
    "        # self.r6  = torch.nn.Linear(300, 150, bias=True)\n",
    "\n",
    "        self.a = torch.nn.LeakyReLU() # negative_slope=0.3)\n",
    "        self.a2 = torch.nn.LeakyReLU() #negative_slope=0.6)\n",
    "\n",
    "        self.s = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1  = self.a(self.l1(x))\n",
    "        x2   = self.a(self.l2(x1))\n",
    "        x3   = self.a(self.l3(x2))\n",
    "        x4  = self.a(self.l4(x3)) \n",
    "        x5   = self.a(self.l5(x4))\n",
    "        x6  = self.a(self.l6(x5))\n",
    "        x7  = self.a(self.l7(x6))\n",
    "        x8  = self.a(self.l8(x7))\n",
    "        x9  = self.a(self.l9(x8))\n",
    "        x10  = self.a(self.l10(x9)) \n",
    "        x11  = self.a(self.l11(x10))\n",
    "        x12  = self.a(self.l12(x11))\n",
    "        x13  = self.a(self.l13(x12)) \n",
    "        x14 = self.a(self.l14(x13))\n",
    "        x15 = self.a(self.l15(x14))\n",
    "        x16 = self.a(self.l16(x15)) \n",
    "        x17 = self.a(self.l17(x16))\n",
    "        x18 = self.a(self.l18(x17))\n",
    "        x19 = self.a(self.l19(x18)) \n",
    "        x20 = self.a(self.l20(x19))\n",
    "        x21 = self.a(self.l21(x20))\n",
    "        x22 = self.a(self.l22(x21)) \n",
    "        x23 = self.a(self.l23(x22))\n",
    "        \n",
    "        a = np.random.randint(2,11)\n",
    "        # print(a)\n",
    "        self.l0 = torch.nn.Linear(eval('model.l{}.out_features'.format(a)), 500).to(device)\n",
    "        self.l0.weight.data = torch.ones_like(self.l0.weight)\n",
    "        self.l0.weight.requires_grad = False\n",
    "        self.l0.bias.data = torch.ones_like(self.l0.bias)\n",
    "        self.l0.bias.requires_grad = False\n",
    "        \n",
    "        \n",
    "        x24 = self.a(self.l0(eval('x{}'.format(a))))\n",
    "        x24 = self.a(self.l01(x24))\n",
    "        x24 = self.a(self.l02(x24))\n",
    "        \n",
    "        return x23, x24, a\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# print(device)\n",
    "model = AirfoilPredictor().to(device)\n",
    "# print(model)\n",
    "loss_f = torch.nn.MSELoss() #reduction='sum')\n",
    "loss_m = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = model.eta, betas=[model.beta_1, model.beta_2])\n",
    "\n",
    "# load_mode;\n",
    "# model.load_state_dict(torch.load('net_20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(af_points='tr_af_points.pickle',\n",
    "              af_labels='tr_af_labels.pickle',\n",
    "              label_afs='tr_label_afs.pickle'):\n",
    "\n",
    "    with open(af_points, 'rb') as fid:\n",
    "        af_data_dic = pickle.load(fid, encoding='latin1')\n",
    "    with open(af_labels, 'rb') as fid:\n",
    "        af_label = pickle.load(fid, encoding='latin1')\n",
    "    with open(label_afs, 'rb') as fid:\n",
    "        label_af = pickle.load(fid, encoding='latin1')\n",
    "\n",
    "    return af_data_dic, af_label, label_af\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size))\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_dev_test_sets(df_loc='tr_raw_af_data', s_train=0.99,\n",
    "                        alpha_range=[-5, 25], Re_range=[1000000, 10000000],\n",
    "                        inputs_list=['af', 're', 'a'], outputs_list=['cd'],\n",
    "                        normalize=['re', 'a', 'cl', 'cd', 'cdp', 'cm']):\n",
    "    sets = {}\n",
    "\n",
    "    # create input\n",
    "    data_train = pd.read_csv(df_loc+'.txt')\n",
    "    index = data_train.index\n",
    "    h = data_train.copy()\n",
    "    # modify data\n",
    "    h = h[(h['a'] > alpha_range[0]) & (h['a'] < alpha_range[1])]\n",
    "    h = h[(h['re'] > Re_range[0]) & (h['re'] < Re_range[1])]\n",
    "    # normalize the desired columns\n",
    "    n_params = {}\n",
    "\n",
    "\n",
    "    '''\n",
    "    for col in normalize:\n",
    "        mu = h[col].mean()\n",
    "        sigma = h[col].std()\n",
    "        h[col] = (h[col] - mu) / sigma\n",
    "        n_params[col] = {'mu': mu, 'sigma': sigma}\n",
    "    '''\n",
    "    h['re'] = h['re'] / 8000000.\n",
    "    h['a'] = h['a'] / 30.\n",
    "\n",
    "    # lets make the outputs bigger so that we decrease the error more rapidly and make the error smaller 10.11.19\n",
    "    ###\n",
    "    h['cl'] = (h['cl'] / 4.8) + 0.5\n",
    "    h['cd'] = (h['cd']) / 0.2\n",
    "    h['cdp'] = (h['cdp']) / 0.2\n",
    "    h['cm'] = (h['cm'] / 0.8) + 0.5\n",
    "\n",
    "    #for nnn in ['cl', 'cd', 'cdp', 'cm']:\n",
    "    #    print(nnn, h[nnn].min(), h[nnn].max())\n",
    "    ###\n",
    "\n",
    "    # shuffle the data three times\n",
    "    # shuffle the data three times\n",
    "    h = h.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "    h = h.reindex(np.random.permutation(h.index)).reset_index(drop=True)\n",
    "    ## third shuffle\n",
    "    inputs_train = h.sample(frac=s_train)\n",
    "    remaining = h.drop(inputs_train.index)\n",
    "    inputs_train = inputs_train.reset_index(drop=True)\n",
    "    inputs_dev = remaining.sample(frac=(1 - s_train)).reset_index(drop=True)\n",
    "\n",
    "    # create the train and dev sets\n",
    "    sets['x_train'] = inputs_train[inputs_list].values.transpose()\n",
    "    sets['y_train'] = inputs_train[outputs_list].values.transpose()\n",
    "    sets['m_train'] = sets['x_train'].shape[1]\n",
    "\n",
    "    # just not save the dev set 10.11.19\n",
    "    ## commented out this section\n",
    "\n",
    "    #sets['x_dev'] = inputs_dev[inputs_list].values.transpose()\n",
    "    #sets['y_dev'] = inputs_dev[outputs_list].values.transpose()\n",
    "    #sets['m_dev'] = sets['x_dev'].shape[1]\n",
    "\n",
    "    ##\n",
    "\n",
    "    \"\"\"\n",
    "    # create the test set\n",
    "    data_test = pd.read_csv(df_loc+'_test.txt')\n",
    "    h = data_test.copy()\n",
    "    # modify data\n",
    "    h = h[(h['a'] > alpha_range[0]) & (h['a'] < alpha_range[1])]\n",
    "    h = h[(h['re'] > Re_range[0]) & (h['re'] < Re_range[1])]\n",
    "    '''\n",
    "    for col in normalize:\n",
    "        h[col] = (h[col] - n_params[col]['mu']) / n_params[col]['sigma']\n",
    "    '''\n",
    "    h['re'] = h['re'] / 8000000.\n",
    "    h['a'] = h['a'] / 30.\n",
    "\n",
    "    sets['x_test'] = h[inputs_list].values.transpose()\n",
    "    sets['y_test'] = h[outputs_list].values.transpose()\n",
    "    sets['m_test'] = sets['x_test'].shape[1]\n",
    "    \"\"\"\n",
    "    return n_params, sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbatch = 3411.000000\n",
      "Epoch [1/400], Loss: 1717818557477732864.0000 bachsize: 1000, time 79.435857, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [2/400], Loss: 20605595.3574 bachsize: 1000, time 79.082584, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [3/400], Loss: 9345569.2425 bachsize: 1000, time 79.641071, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [4/400], Loss: 59380274.8338 bachsize: 1000, time 79.247832, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [5/400], Loss: 18681147.3422 bachsize: 1000, time 79.863070, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [6/400], Loss: 27346998998518784.0000 bachsize: 1000, time 79.627296, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [7/400], Loss: 1841874574959547.5000 bachsize: 1000, time 79.705193, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [8/400], Loss: 2017046.6631 bachsize: 1000, time 79.874769, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [9/400], Loss: 689864.1386 bachsize: 1000, time 79.938223, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [10/400], Loss: 436526.6067 bachsize: 1000, time 79.969743, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [11/400], Loss: 352650.9961 bachsize: 1000, time 80.198689, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [12/400], Loss: 300981.5077 bachsize: 1000, time 79.969514, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [13/400], Loss: 276974.6732 bachsize: 1000, time 79.850351, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [14/400], Loss: 264178.2645 bachsize: 1000, time 79.851765, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [15/400], Loss: 257801.0789 bachsize: 1000, time 80.110617, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [16/400], Loss: 253761.1410 bachsize: 1000, time 79.866689, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [17/400], Loss: 251979.9874 bachsize: 1000, time 80.012655, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [18/400], Loss: 250832.9029 bachsize: 1000, time 80.015861, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [19/400], Loss: 251013.7809 bachsize: 1000, time 79.883074, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [20/400], Loss: 250558.5786 bachsize: 1000, time 79.723293, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [21/400], Loss: 249867.5537 bachsize: 1000, time 79.901056, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [22/400], Loss: 251008.1265 bachsize: 1000, time 79.943373, 3411.000000\n",
      "nbatch = 3411.000000\n",
      "Epoch [23/400], Loss: 250747.0047 bachsize: 1000, time 79.937334, 3411.000000\n",
      "nbatch = 3411.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dccf34372ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# convert the x_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx_af\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx_af\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maf_data_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_af\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_af\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mminibatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_af\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-dccf34372ead>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# convert the x_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx_af\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx_af\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maf_data_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_af\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_af\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mminibatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_af\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load everything\n",
    "af_data_dic, af_label, label_af = load_data()\n",
    "#n_params, sets = train_dev_test_sets()\n",
    "\n",
    "seed = 0\n",
    "costs = []\n",
    "b_s = 1000\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    t = time.time()\n",
    "    epoch_cost = 0.\n",
    "    #if (epoch % 30) == 0 and b_s < 200 and epoch !=0:\n",
    "    model.eta *= math.exp(-epoch/20)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = model.eta, betas=[model.beta_1, model.beta_2])\n",
    "    n_params, sets = train_dev_test_sets()\n",
    "    n_batch = int(sets['m_train']/b_s)\n",
    "    print('nbatch = %f' %(n_batch))\n",
    "    seed = seed + 1\n",
    "    minibatches = random_mini_batches(sets['x_train'], sets['y_train'], b_s, seed)\n",
    "    # offload the n params and sets\n",
    "    n_params, sets = [], []\n",
    "    for minibatch in minibatches:\n",
    "        (minibatch_x, minibatch_y) = minibatch\n",
    "        # convert the x_s\n",
    "        x_af = minibatch_x[0, :].astype(int)\n",
    "        x_af = np.array([af_data_dic[label_af[i]]['input'].flatten() for i in x_af]).transpose()\n",
    "        x_temp = minibatch_x[1:3, :]\n",
    "        minibatch_x = np.concatenate((x_af, x_temp), axis=0)\n",
    "        minibatch_x = torch.from_numpy(minibatch_x.T).float().to(device)\n",
    "        minibatch_y = torch.from_numpy(minibatch_y.T).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o1, o2, o3 = model(minibatch_x)\n",
    "        loss = loss_f(o1, minibatch_y) + loss_m(o2, minibatch_y) \n",
    "        minibatch_cost = loss.tolist() * 100000000.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_cost += minibatch_cost / n_batch\n",
    "\n",
    "    print('Epoch [%d/%d], Loss: %.4f bachsize: %d, time %f, %f'  %(epoch+1, num_epochs, epoch_cost, b_s, (time.time()-t), n_batch))\n",
    "    #if (epoch % 10) == 0 or epoch == num_epochs-1:\n",
    "    torch.save(model.state_dict(),'net_{}'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
