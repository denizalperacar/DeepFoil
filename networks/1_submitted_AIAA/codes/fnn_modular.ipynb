{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(af_points='af_points.pickle',\n",
    "              af_labels='af_labels.pickle',\n",
    "              label_afs='label_afs.pickle'):\n",
    "    \n",
    "    with open(af_points, 'rb') as fid:\n",
    "        af_data_dic = pickle.load(fid, encoding='latin1')\n",
    "    with open(af_labels, 'rb') as fid:\n",
    "        af_label = pickle.load(fid, encoding='latin1')\n",
    "    with open(label_afs, 'rb') as fid:\n",
    "        label_af = pickle.load(fid, encoding='latin1')\n",
    "    \n",
    "    return af_data_dic, af_label, label_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameters(lamda = 0.00001, learning_rate = 0.0001, \n",
    "                     epochs = 5000, batch_size = 1024, \n",
    "                     beta_1 = 0.95, beta_2 = 0.99,\n",
    "                     layers = [512, 256, 200, 128, 4]):\n",
    "    \n",
    "    hp = {'lamda': lamda, \n",
    "          'lr': learning_rate,\n",
    "          'epochs': epochs,\n",
    "          'batch_size': batch_size,\n",
    "          'adams_beta':[beta_1, beta_2],\n",
    "          'layers': layers}\n",
    "    \n",
    "    \n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_sets(df_loc='raw_af_data.txt', s_train=0.9, s_dev=0.05, \n",
    "                        alpha_range= [-5, 15], Re_range=[500000,10000000],\n",
    "                        inputs_list=['af', 're', 'a'], outputs_list=['cl', 'cd', 'cdp', 'cm'],\n",
    "                        normalize= ['re', 'a', 'cl', 'cd', 'cdp', 'cm']):\n",
    "    \n",
    "    sets = {}\n",
    "    \n",
    "    # create input\n",
    "    data = pd.read_csv(df_loc)\n",
    "    index = data.index\n",
    "    h = data.copy()\n",
    "    # modify data\n",
    "    h = h[(h['a'] > alpha_range[0]) & (h['a'] < alpha_range[1])]\n",
    "    h = h[(h['re'] > Re_range[0]) & (h['re'] < Re_range[1])]\n",
    "    # normalize the desired columns\n",
    "    n_params = {}\n",
    "    for col in normalize:\n",
    "        mu = h[col].mean()\n",
    "        sigma = h[col].std()\n",
    "        h[col] = (h[col] - mu) / sigma\n",
    "        n_params[col] = {'mu':mu, 'sigma':sigma}\n",
    "    \n",
    "    # shuffle the data three times\n",
    "    # shuffle the data three times\n",
    "    h = h.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "    h = h.reindex(np.random.permutation(h.index)).reset_index(drop=True)\n",
    "    ## third shuffle\n",
    "    inputs_train = h.sample(frac=s_train)\n",
    "    remaining = h.drop(inputs_train.index)\n",
    "    inputs_train = inputs_train.reset_index(drop=True)\n",
    "    inputs_dev = remaining.sample(frac=(s_dev/(1-s_train)))\n",
    "    inputs_test = remaining.drop(inputs_dev.index).reset_index(drop=True)\n",
    "    inputs_dev = inputs_dev.reset_index(drop=True)\n",
    "    \n",
    "    sets['x_train'] = inputs_train[inputs_list].values.transpose()\n",
    "    sets['y_train'] = inputs_train[outputs_list].values.transpose()\n",
    "    sets['m_train'] = sets['x_train'].shape[1]\n",
    "\n",
    "    sets['x_dev'] = inputs_dev[inputs_list].values.transpose()\n",
    "    sets['y_dev'] = inputs_dev[outputs_list].values.transpose()\n",
    "    sets['m_dev'] = sets['x_dev'].shape[1]\n",
    "\n",
    "    sets['x_test'] = inputs_test[inputs_list].values.transpose()\n",
    "    sets['y_test'] = inputs_test[outputs_list].values.transpose()\n",
    "    sets['m_test'] = sets['x_test'].shape[1]\n",
    "    \n",
    "    \n",
    "    return n_params, sets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(sets, label_af, layers=[512, 256, 200, 128, 4], lamda=0.8, seed=0):\n",
    "    \n",
    "    network = {}\n",
    "    tf.reset_default_graph()\n",
    "    network['n'] = af_data_dic[label_af[0]]['input'].shape[1] + 2\n",
    "    network['A0'] = tf.placeholder('float64', shape=(network['n'], None))\n",
    "    network['y'] = tf.placeholder('float64', shape=(sets['y_train'].shape[0], None))\n",
    "    a = [network['n']]\n",
    "    a.extend(layers)\n",
    "    layers = a\n",
    "    \n",
    "    for i in range(1, len(layers)):\n",
    "    \n",
    "        network['W'+str(i)] = tf.get_variable(\"W\"+str(i), (layers[i], layers[i-1]),\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(seed=seed),\n",
    "                                              dtype=tf.float64)\n",
    "        network['b'+str(i)] = tf.get_variable(\"b\"+str(i), [layers[i], 1], \n",
    "                                              initializer=tf.zeros_initializer(), dtype=tf.float64)\n",
    "        network['Z'+str(i)] = tf.add(tf.matmul(network['W'+ str(i)], network['A'+ str(i-1)]), \n",
    "                                     network['b'+ str(i)])\n",
    "        if i != len(layers)-1:\n",
    "            network['A'+str(i)] = tf.nn.leaky_relu(network['Z'+str(i)])\n",
    "        else:\n",
    "            network['A'+str(i)] = network['Z'+str(i)]\n",
    "        if i == 1:\n",
    "            network['reg'] = tf.nn.l2_loss(network['W'+str(i)])\n",
    "        else:\n",
    "            network['reg'] += tf.nn.l2_loss(network['W'+str(i)])\n",
    "    \n",
    "    # calculate cost\n",
    "    network['cost'] = tf.reduce_sum((network['y']-network['A'+str(len(layers)-1)])**2.)\n",
    "    # compute reqularized loss\n",
    "    network['cost'] = tf.reduce_mean(network['cost'] + lamda * network['reg'])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 11:40:28.125597 140114040710976 deprecation.py:323] From /home/ariya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "af_data_dic, af_label, label_af = load_data()\n",
    "hp = hyper_parameters()\n",
    "n_params, sets = train_dev_test_sets()\n",
    "tf.reset_default_graph()\n",
    "network = create_network(sets, label_af, hp['layers'], hp['lamda'])\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=hp['lr'], \n",
    "                                   beta1=hp['adams_beta'][0], \n",
    "                                   beta2=hp['adams_beta'][0]).minimize(network['cost'])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904.8983334016019\n",
      "137.63075106209908\n",
      "52.21754732163326\n",
      "28.91767798057524\n",
      "18.914368017840108\n",
      "14.928781690058344\n",
      "13.334384431061675\n",
      "12.38603881033648\n",
      "11.576282986802422\n",
      "10.829238356748544\n",
      "10.311410572558263\n",
      "9.829436806466811\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7e7b550647c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mminibatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_af\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             _, minibatch_cost = sess.run([optimizer, network['cost']], feed_dict={network['A0']: minibatch_x, \n\u001b[0;32m---> 19\u001b[0;31m                                                                                   network['y']: minibatch_y})\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "costs = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(hp['epochs']):\n",
    "        epoch_cost = 0.\n",
    "        n_batch = int(sets['m_train']/hp['batch_size'])\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(sets['x_train'], sets['y_train'], hp['batch_size'], seed)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_x, minibatch_y) = minibatch\n",
    "            # convert the x_s\n",
    "            x_af = minibatch_x[0, :].astype(int)\n",
    "            x_af = np.array([af_data_dic[label_af[i]]['input'].flatten() for i in x_af]).transpose()\n",
    "            x_temp = minibatch_x[1:3, :]\n",
    "            minibatch_x = np.concatenate((x_af, x_temp), axis=0)\n",
    "            _, minibatch_cost = sess.run([optimizer, network['cost']], feed_dict={network['A0']: minibatch_x, \n",
    "                                                                                  network['y']: minibatch_y})\n",
    "            epoch_cost += minibatch_cost / n_batch\n",
    "\n",
    "        # Print the cost every epoch\n",
    "        if epoch % 1 == 0:\n",
    "            fid = open('res.txt', 'a')\n",
    "            fid.write(\"Cost after epoch {}: {}\\n\".format(epoch, epoch_cost))\n",
    "            fid.close()\n",
    "        if epoch % 1 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "            print(epoch_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
