{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirfoilPredictor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AirfoilPredictor, self).__init__()\n",
    "        self.net = {} \n",
    "        self.d = 326\n",
    "        self.lamda = 0.00001\n",
    "        self.eta = 0.0001\n",
    "        self.beta_1 = 0.95\n",
    "        self.beta_2 = 0.99\n",
    "        self.l1 = torch.nn.Linear(self.d, 512, bias=True)\n",
    "        self.l2 = torch.nn.Linear(512,256, bias=True)\n",
    "        self.l3 = torch.nn.Linear(256, 200, bias=True)\n",
    "        self.l4 = torch.nn.Linear(200, 128, bias=True)\n",
    "        self.l5 = torch.nn.Linear(128, 4, bias=True)\n",
    "        self.a = torch.nn.LeakyReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.a(self.l1(x))\n",
    "        x = self.a(self.l2(x))\n",
    "        x = self.a(self.l3(x))\n",
    "        x = self.a(self.l4(x))\n",
    "        x = self.a(self.l5(x))\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')    \n",
    "model = AirfoilPredictor().to(device)\n",
    "loss_f = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = model.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(af_points='tr_af_points.pickle',\n",
    "              af_labels='tr_af_labels.pickle',\n",
    "              label_afs='tr_label_afs.pickle'):\n",
    "    \n",
    "    with open(af_points, 'rb') as fid:\n",
    "        af_data_dic = pickle.load(fid, encoding='latin1')\n",
    "    with open(af_labels, 'rb') as fid:\n",
    "        af_label = pickle.load(fid, encoding='latin1')\n",
    "    with open(label_afs, 'rb') as fid:\n",
    "        label_af = pickle.load(fid, encoding='latin1')\n",
    "    \n",
    "    return af_data_dic, af_label, label_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size))\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_dev_test_sets(df_loc='tr_raw_af_data', s_train=0.95,\n",
    "                        alpha_range=[-5, 15], Re_range=[500000, 10000000],\n",
    "                        inputs_list=['af', 're', 'a'], outputs_list=['cl', 'cd', 'cdp', 'cm'],\n",
    "                        normalize=['re', 'a', 'cl', 'cd', 'cdp', 'cm']):\n",
    "    sets = {}\n",
    "    \n",
    "    # create input\n",
    "    data_train = pd.read_csv(df_loc+'.txt')\n",
    "    index = data_train.index\n",
    "    \n",
    "    h = data_train.copy()\n",
    "    # modify data\n",
    "    h = h[(h['a'] > alpha_range[0]) & (h['a'] < alpha_range[1])]\n",
    "    h = h[(h['re'] > Re_range[0]) & (h['re'] < Re_range[1])]\n",
    "    # normalize the desired columns\n",
    "    n_params = {}\n",
    "\n",
    "\n",
    "    '''\n",
    "    for col in normalize:\n",
    "        mu = h[col].mean()\n",
    "        sigma = h[col].std()\n",
    "        h[col] = (h[col] - mu) / sigma\n",
    "        n_params[col] = {'mu': mu, 'sigma': sigma}\n",
    "    '''\n",
    "    h['re'] = h['re'] / 8000000.\n",
    "    h['a'] = h['a'] / 30.\n",
    "    # shuffle the data three times\n",
    "    # shuffle the data three times\n",
    "    h = h.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "    h = h.reindex(np.random.permutation(h.index)).reset_index(drop=True)\n",
    "    ## third shuffle\n",
    "    inputs_train = h.sample(frac=s_train)\n",
    "    remaining = h.drop(inputs_train.index)\n",
    "    inputs_train = inputs_train.reset_index(drop=True)\n",
    "    inputs_dev = remaining.sample(frac=(1 - s_train)).reset_index(drop=True)\n",
    "\n",
    "    # create the train and dev sets\n",
    "    sets['x_train'] = inputs_train[inputs_list].values.transpose()\n",
    "    sets['y_train'] = inputs_train[outputs_list].values.transpose()\n",
    "    sets['m_train'] = sets['x_train'].shape[1]\n",
    "\n",
    "    sets['x_dev'] = inputs_dev[inputs_list].values.transpose()\n",
    "    sets['y_dev'] = inputs_dev[outputs_list].values.transpose()\n",
    "    sets['m_dev'] = sets['x_dev'].shape[1]\n",
    "    \"\"\"\n",
    "    # create the test set\n",
    "    data_test = pd.read_csv(df_loc+'_test.txt')\n",
    "    h = data_test.copy()\n",
    "    # modify data\n",
    "    h = h[(h['a'] > alpha_range[0]) & (h['a'] < alpha_range[1])]\n",
    "    h = h[(h['re'] > Re_range[0]) & (h['re'] < Re_range[1])]\n",
    "    '''\n",
    "    for col in normalize:\n",
    "        h[col] = (h[col] - n_params[col]['mu']) / n_params[col]['sigma']\n",
    "    '''\n",
    "    h['re'] = h['re'] / 8000000.\n",
    "    h['a'] = h['a'] / 30.\n",
    "\n",
    "    sets['x_test'] = h[inputs_list].values.transpose()\n",
    "    sets['y_test'] = h[outputs_list].values.transpose()\n",
    "    sets['m_test'] = sets['x_test'].shape[1]\n",
    "    \"\"\"\n",
    "    return n_params, sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load everything\n",
    "af_data_dic, af_label, label_af = load_data()\n",
    "n_params, sets = train_dev_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seed = 0\n",
    "costs = []\n",
    "b_s = 1\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_cost = 0.\n",
    "    if (epoch % 5) == 0 and b_s <= 2049:\n",
    "        b_s = b_s * 2\n",
    "    n_batch = int(sets['m_train']/b_s)\n",
    "    seed = seed + 1\n",
    "    minibatches = random_mini_batches(sets['x_train'], sets['y_train'], b_s, seed)\n",
    "    \n",
    "    for minibatch in minibatches:\n",
    "        (minibatch_x, minibatch_y) = minibatch\n",
    "        # convert the x_s\n",
    "        x_af = minibatch_x[0, :].astype(int)\n",
    "        x_af = np.array([af_data_dic[label_af[i]]['input'].flatten() for i in x_af]).transpose()\n",
    "        x_temp = minibatch_x[1:3, :]\n",
    "        minibatch_x = np.concatenate((x_af, x_temp), axis=0)\n",
    "        minibatch_x = torch.from_numpy(minibatch_x.T).float().to(device)\n",
    "        minibatch_y = torch.from_numpy(minibatch_y.T).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(minibatch_x)\n",
    "        loss = loss_f(output, minibatch_y)\n",
    "        minibatch_cost = loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_cost += minibatch_cost / n_batch\n",
    "    if (epoch % 20) == 0:\n",
    "        torch.save(model.state_dict(),'net_{}'.format(epoch+1))\n",
    "    print('Epoch [%d/%d], Loss: %.4f bachsize: %d'  %(epoch+1, num_epochs, epoch_cost, b_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'net_{}'.format('f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('net_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
